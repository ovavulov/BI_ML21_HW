{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным и сравнить его эффективность с ансамблями из самых популярных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "from hyperopt import fmin, hp, Trials, anneal\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "SEED = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (3 балла)\n",
    "Реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание. Используйте реализацию дерева из HW3.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    \n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, \n",
    "                 max_features=\"auto\", n_estimators=10, score_fn=accuracy_score):\n",
    "        assert criterion in [\"gini\", \"entropy\"]\n",
    "        assert max_depth is None or max_depth > 0 and int(max_depth) == max_depth\n",
    "        assert min_samples_leaf > 0 and int(min_samples_leaf) == min_samples_leaf\n",
    "        assert max_features == \"auto\" or max_depth > 0 and int(max_depth) == max_depth\n",
    "        assert n_estimators > 0 and int(n_estimators) == n_estimators\n",
    "        self.tree_params = {\n",
    "            \"criterion\": criterion,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"min_samples_leaf\": min_samples_leaf\n",
    "        }\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.trees = []\n",
    "        self.oob = []\n",
    "        self.score_fn = score_fn\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        from tree import DecisionTreeClassifier\n",
    "        # случайная функция, которая выдает в среднем корень из общего числа признаков\n",
    "        subspace_fn = lambda n: int(np.sqrt(n)) + np.random.binomial(1, np.sqrt(n) - int(np.sqrt(n)))\n",
    "        subspace = subspace_fn if self.max_features == \"auto\" else self.max_features\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # бутстреп сэмплинг\n",
    "            idxs = np.random.choice(X.index, size=len(X))\n",
    "            X_sampled = X.loc[idxs, :].reset_index(drop=True)\n",
    "            y_sampled = y.loc[idxs].reset_index(drop=True)\n",
    "            # обучаем дерево на случайных подпространствах\n",
    "            tree = DecisionTreeClassifier(**self.tree_params)\n",
    "            tree.fit(X_sampled, y_sampled, subspace=subspace)\n",
    "            self.trees.append(tree)\n",
    "            # оцениваем ошибку out-of-bag\n",
    "            idxs_out = list(set(X.index) - set(idxs))\n",
    "            X_oob = X.loc[idxs_out, :]\n",
    "            y_oob = y.loc[idxs_out]\n",
    "            self.oob.append(self.score_fn(y_oob, tree.predict(X_oob)))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        votes = []\n",
    "        for tree in self.trees:\n",
    "            votes.append(tree.predict(X))\n",
    "        votes = pd.concat(votes, axis=1).values\n",
    "        y_pred = stats.mode(votes, axis=1)[0].reshape(1, -1)[0]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Оптимизируйте по `AUC` на кроссвалидации (размер валидационной выборки - 20%) параметры своей реализации `Random Forest`: \n",
    "\n",
    "максимальную глубину деревьев из [2, 3, 5, 7, 10], количество деревьев из [5, 10, 20, 30, 50, 100]. \n",
    "\n",
    "Постройте `ROC` кривую (и выведите `AUC` и `accuracy`) для лучшего варианта.\n",
    "\n",
    "Подсказка: можно построить сразу 100 деревьев глубины 10, а потом убирать деревья и\n",
    "глубину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return pd.DataFrame(np.array(X)), pd.Series(np.array(y))\n",
    "\n",
    "X, y = synthetic_dataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_rf_params(X, y, n_splits, space, max_evals):\n",
    "    \n",
    "    def cross_val_loss(params, X=X, y=y, n_splits=n_splits):\n",
    "        params[\"n_estimators\"] = int(params[\"n_estimators\"])\n",
    "        params[\"max_depth\"] = int(params[\"max_depth\"])\n",
    "        model = RandomForestClassifier(**params)\n",
    "        skf = StratifiedKFold(n_splits=n_splits)\n",
    "        scores = []\n",
    "        for train_index, val_index in skf.split(X, y):\n",
    "            X_t, X_v = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "            y_t, y_v = y.iloc[train_index], y.iloc[val_index]\n",
    "            model.fit(X_t, y_t)\n",
    "            y_p = model.predict(X_v)\n",
    "            scores.append(accuracy_score(y_v, y_p))\n",
    "        return -np.mean(scores)\n",
    "    \n",
    "    return fmin(cross_val_loss, space, anneal.suggest, max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:50<00:00, 22.05s/trial, best loss: -1.0]\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 10, 1),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 5, 10, 5)\n",
    "}\n",
    "\n",
    "best_params = find_best_rf_params(X_train, y_train, n_splits=3, space=space, max_evals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 0, 'max_depth': 7.0, 'n_estimators': 5.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (3 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest выглядит так:\n",
    "1. Посчитать out-of-bag ошибку предсказания `err_oob` (https://en.wikipedia.org/wiki/Out-of-bag_error)\n",
    "2. Перемешать значения признака `j` у объектов выборки (у каждого из объектов изменится значение признака `j` на какой-то другой)\n",
    "3. Посчитать out-of-bag ошибку (`err_oob_j`) еще раз.\n",
    "4. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте решение на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте, какие признаки важны для датасетов cancer и spam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ваш ответ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (2 балла)\n",
    "В качестве альтернативы попробуем библиотечные реализации ансамблей моделей. \n",
    "\n",
    "1. [CatBoost](https://catboost.ai/docs/)\n",
    "2. [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n",
    "3. [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установите необходимые библиотеки. \n",
    "Возможно, потребуется установка дополнительных пакетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "!pip install catboost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, как и реализованный нами RandomForest, примените модели для наших датасетов.\n",
    "\n",
    "Для стандартного набора параметров у каждой модели нарисуйте `ROC` кривую и выведите `AUC` и `accuracy`.\n",
    "\n",
    "Посчитайте время обучения каждой модели (можно использовать [timeit magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)). \n",
    "\n",
    "Сравните метрики качества и скорость обучения моделей. Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ваш ответ_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
